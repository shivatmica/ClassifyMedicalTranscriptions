{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YpwIV8hBtSzZ"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import warnings\n",
        "import altair as alt\n",
        "import pickle\n",
        "import string\n",
        "import spacy\n",
        "import nltk \n",
        "import re\n",
        "\n",
        "from sklearn.naive_bayes import *\n",
        "from sklearn.ensemble import *\n",
        "from sklearn.neighbors import *\n",
        "from sklearn.tree import *\n",
        "from sklearn.calibration import *\n",
        "from sklearn.linear_model import *\n",
        "from sklearn.multiclass import *\n",
        "from sklearn.svm import *\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from nltk.stem import WordNetLemmatizer \n",
        "from collections import Counter\n",
        "\n",
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, auc, roc_curve\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer, HashingVectorizer\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, KFold, GridSearchCV\n",
        "from sklearn.pipeline import Pipeline, make_pipeline"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "try:\n",
        "    nlp = spacy.load('en_core_web_lg')\n",
        "except Exception as e:\n",
        "    !spacy download en_core_web_lg\n",
        "    nlp = spacy.load('en_core_web_lg')"
      ],
      "metadata": {
        "id": "VWLbwgBzxIaZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/overview-of-recordings.csv')"
      ],
      "metadata": {
        "id": "Tzk-zjbft4e7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "kvBESfWuuDIw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_text = df[['phrase', 'prompt']]"
      ],
      "metadata": {
        "id": "GduKJdLCuD3P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_text.isna().sum()\n",
        "df_text.dropna(inplace = True)"
      ],
      "metadata": {
        "id": "HqBUOCeSuIYw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_text"
      ],
      "metadata": {
        "id": "IzRFKeBAuoGq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.duplicated().sum()"
      ],
      "metadata": {
        "id": "ggOFDMwiuqqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "negative = [\"haven't\", \"won't\", \"mightn't\", \"not\", \"doesn't\", \"needn't\", \"wouldn'\", \"weren't\", \"didn't\", \"hadn't\", \"wouldn't\", \"wasn't\", \"don't\", \"isn'\", \"no\", \"shan't\", \"aren't\", \"couldn't\", \"mustn't\", \"cannot\"]\n",
        "STOP_WORDS = set([i for i in STOP_WORDS if i not in negative])\n",
        "df_text['no_of_characters'] = df_text['phrase'].apply(len)\n",
        "df_text['no_of_tokens'] = df_text['phrase'].apply(lambda w: len(w.split()))\n",
        "df_text['average_length_of_word'] = df_text['phrase'].apply(lambda w: np.mean([len(a) for a in w.split()]))\n",
        "df_text['no_stop_words'] = df_text['phrase'].apply(lambda w: len([t for t in w.split() if t not in STOP_WORDS]))\n",
        "df_text"
      ],
      "metadata": {
        "id": "BbBOOwNwu_0Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_txt(docs):\n",
        "    speech_words = nlp(docs)\n",
        "    lower_text = [w.text.lower() for w in speech_words]\n",
        "    re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
        "    stripped = [re_punc.sub('', w) for w in lower_text]\n",
        "    words = [word for word in stripped if word.isalpha()]\n",
        "    words = [w for w in words if not w in  list(STOP_WORDS)]\n",
        "    words = [word for word in words if len(word) > 2]\n",
        "    lem_words = [word.lemma_ for word in nlp(' '.join(words))]\n",
        "    combined_text = ' '.join(lem_words)\n",
        "    return combined_text\n",
        "\n",
        "# Cleaning the text data\n",
        "df_text['cleaned_phrase'] = df_text['phrase'].apply(clean_txt)\n",
        "df_text"
      ],
      "metadata": {
        "id": "RrRNcRtP0LIR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_text"
      ],
      "metadata": {
        "id": "b1nLFsVS4Qhz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Spot-Check Normalized Text Models\n",
        "def NormalizedTextModel(nameOfvect):\n",
        "    if nameOfvect == 'countvect':\n",
        "        vectorizer = CountVectorizer()\n",
        "    elif nameOfvect =='tfvect':\n",
        "        vectorizer = TfidfVectorizer()\n",
        "    elif nameOfvect == 'hashvect':\n",
        "        vectorizer = HashingVectorizer()\n",
        "\n",
        "    pipelines = []\n",
        "    pipelines.append((nameOfvect+'MultinomialNB'  , Pipeline([('Vectorizer', vectorizer),('NB'  , MultinomialNB())])))\n",
        "    pipelines.append((nameOfvect+'CCCV' , Pipeline([('Vectorizer', vectorizer),('CCCV' , CalibratedClassifierCV())])))\n",
        "    pipelines.append((nameOfvect+'KNN' , Pipeline([('Vectorizer', vectorizer),('KNN' , KNeighborsClassifier())])))\n",
        "    pipelines.append((nameOfvect+'CART', Pipeline([('Vectorizer', vectorizer),('CART', DecisionTreeClassifier())])))\n",
        "    pipelines.append((nameOfvect+'PAC'  , Pipeline([('Vectorizer', vectorizer),('PAC'  , PassiveAggressiveClassifier())])))\n",
        "    pipelines.append((nameOfvect+'SVM' , Pipeline([('Vectorizer', vectorizer),('RC' , RidgeClassifier())])))\n",
        "    pipelines.append((nameOfvect+'AB'  , Pipeline([('Vectorizer', vectorizer),('AB'  , AdaBoostClassifier())])  ))\n",
        "    pipelines.append((nameOfvect+'GBM' , Pipeline([('Vectorizer', vectorizer),('GMB' , GradientBoostingClassifier())])))\n",
        "    pipelines.append((nameOfvect+'RF'  , Pipeline([('Vectorizer', vectorizer),('RF'  , RandomForestClassifier())])))\n",
        "    pipelines.append((nameOfvect+'ET'  , Pipeline([('Vectorizer', vectorizer),('ET'  , ExtraTreesClassifier())])))\n",
        "    pipelines.append((nameOfvect+'SGD'  , Pipeline([('Vectorizer', vectorizer),('SGD'  , SGDClassifier())])))\n",
        "    pipelines.append((nameOfvect+'OVRC'  , Pipeline([('Vectorizer', vectorizer),('OVRC'  , LogisticRegression())])))\n",
        "    pipelines.append((nameOfvect+'Bagging'  , Pipeline([('Vectorizer', vectorizer),('Bagging'  , BaggingClassifier())])))\n",
        "    pipelines.append((nameOfvect+'NN'  , Pipeline([('Vectorizer', vectorizer),('NN'  , MLPClassifier())])))\n",
        "    return pipelines\n",
        "\n",
        "# Traing model \n",
        "def fit_model(X_train, y_train,models):\n",
        "    # Test options and evaluation metric\n",
        "    num_folds = 10\n",
        "    scoring = 'accuracy'\n",
        "\n",
        "    results = []\n",
        "    names = []\n",
        "    for name, model in models:\n",
        "        kfold = KFold(n_splits=num_folds)\n",
        "        cv_results = cross_val_score(model, X_train, y_train, cv=kfold, scoring=scoring)\n",
        "        results.append(cv_results)\n",
        "        names.append(name)\n",
        "        msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
        "        print(msg)\n",
        "\n",
        "# Split data to training and validation set\n",
        "def read_in_and_split_data(data, features,target):\n",
        "    X = data[features]\n",
        "    y = data[target]\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2, random_state=0)\n",
        "    return X_train, X_test, y_train, y_test\n",
        "\n",
        "X = 'cleaned_phrase'\n",
        "target_class = 'prompt'\n",
        "X_train, X_test, y_train, y_test = read_in_and_split_data(df_text, X, target_class)"
      ],
      "metadata": {
        "id": "MIRH6yT-14o7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "models = NormalizedTextModel('tfvect')\n",
        "models1 = NormalizedTextModel('hashvect')\n",
        "fit_model(X_train, y_train, models)\n",
        "fit_model(X_train, y_train, models1)"
      ],
      "metadata": {
        "id": "6s3VNSs-7XXh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sklearn\n",
        "text_clf = Pipeline([('tfidf', TfidfVectorizer()),('bagging', BaggingClassifier(n_estimators=10))])\n",
        "model = text_clf.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "print(sklearn.metrics.classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "id": "AaeIVMTU7ctn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LCdZOB_O--0G"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}